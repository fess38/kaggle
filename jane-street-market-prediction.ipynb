{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "kviks2l4hc3eyt87l6nfb"
   },
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "okba3s2j2ka81pejlmxh1o"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../input\n",
    "mkdir -p ../output\n",
    "cd ../input\n",
    "\n",
    "export KAGGLE_USERNAME=\"fess38\"\n",
    "export KAGGLE_KEY=\"071966146ec1ebef62023a5efa0574b1\"\n",
    "kaggle competitions download -c jane-street-market-prediction\n",
    "\n",
    "unzip jane-street-market-prediction.zip\n",
    "rm jane-street-market-prediction.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xm2d1ukqreky9p0d2g04fp"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "gyc0u5edjuizta8vsl8hvp",
    "execution_id": "9e442bba-f145-4cea-afdd-a772355bb1b4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cellId": "o37qnzj77wrdfxqvc6e1jv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import sum_models, CatBoostClassifier, CatBoostRegressor, Pool\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "z4g07mdqgheyzvtpn92a5g"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "mpl.rcParams[\"figure.figsize\"] = (11, 5)\n",
    "mpl.rcParams[\"figure.dpi\"]= 100\n",
    "mpl.rcParams[\"lines.linewidth\"] = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2l168l4ogre8w2j8m5wc49"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "tt1yhlytdrwu42ypeukol"
   },
   "outputs": [],
   "source": [
    "input_data_path = \"../input/\"\n",
    "output_data_path = \"../output/\"\n",
    "features = [\"feature_\" + str(i) for i in range(130)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "qywtctpvhzkeomdx3why4o"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    pass\n",
    "\n",
    "random_state = 42\n",
    "seed_all(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "52jqd6zjwdp25uc2uakva2"
   },
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "cellId": "1skv8wzlynspw13r9lrpn"
   },
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def utility_score(date, weight, resp, action):\n",
    "    pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(pi) / np.sqrt(np.sum(pi**2)) * np.sqrt(250 / len(pi))\n",
    "    return min(max(t, 0), 6) * np.sum(pi)\n",
    "\n",
    "def apply_tf_model(df, model):\n",
    "    return np.stack(model.predict(df), axis=1)[0]\n",
    "\n",
    "def estimate_model(df, model, features=features, threshold=0):\n",
    "    expected_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        df[\"action\"].values\n",
    "    )\n",
    "    actual_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        apply_tf_model\n",
    "        (apply_tf_model(df[features], model) > threshold).astype(int)\n",
    "        if \"tensorflow\" in str(type(model))\n",
    "        else (model.predict(df[features], prediction_type=\"RawFormulaVal\") > threshold).astype(int)\n",
    "    )\n",
    "    print(int(expected_score), int(actual_score), round(actual_score / expected_score, 2))\n",
    "    \n",
    "def feature_importances(model, top_n=20):\n",
    "    values = sorted(list(zip(model.feature_names_, model.feature_importances_)), key=lambda x: -x[1])\n",
    "    for value in values[:top_n]:\n",
    "        print(value[0], \": \", str(round(value[1], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "23qjvnnz2p1sslb8os8379"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "mfficxrfsda57fsipqij7n"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_data_path + \"train.csv\")\n",
    "df = df.astype({c: np.float32 for c in df.select_dtypes(include=\"float64\").columns})\n",
    "df[\"action\"] = (df[\"resp\"] > 0).astype(int)\n",
    "\n",
    "features_info = pd.read_csv(input_data_path + \"features.csv\")\n",
    "features_info.set_index(keys=[\"feature\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "422axp6sncgt4g88b7x6w"
   },
   "source": [
    "#### Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "qgfdlsoo9h7w61paq1fvn"
   },
   "outputs": [],
   "source": [
    "def fillna_mean(df):\n",
    "    features_mean = df[features].mean()\n",
    "    df[features] = df[features].fillna(features_mean)\n",
    "    with open(prepared_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "jnjpp98ivlflw88fk4qsj"
   },
   "outputs": [],
   "source": [
    "def fillna_ffill(df):\n",
    "    df[features] = df[features].fillna(method = \"ffill\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "90z64q083eagln57rb5su"
   },
   "outputs": [],
   "source": [
    "def fillna_mean_by_feature_0(df):\n",
    "    features_mean = df[features].groupby(\"feature_0\").mean()\n",
    "    features_mean[\"feature_0\"] = features_mean.index\n",
    "    df.sort_values(by=\"feature_0\", inplace=True)\n",
    "    df[features] = pd.concat([\n",
    "        df[df[\"feature_0\"] == -1][features].fillna(features_mean.loc[-1]),\n",
    "        df[df[\"feature_0\"] == 1][features].fillna(features_mean.loc[1])\n",
    "    ])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    with open(output_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "6bmwkg795e93xd162opb6x"
   },
   "outputs": [],
   "source": [
    "fillna_mean_by_feature_0(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "sipnehvddlc5oea5ywtrvs"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_models = {}\n",
    "tf_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to 2-stage train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = {\n",
    "    \"train_1\": [0, 224],\n",
    "    \"train_2\": [225, 449],\n",
    "    \"val\": [450, 499]\n",
    "}\n",
    "for name, interval in date_splits.items():\n",
    "    df[\"is_\" + name] = df[\"date\"].apply(lambda x: x >= interval[0] and x <= interval[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df[(df[\"is_train_1\"])|(df[\"is_train_2\"])][features])\n",
    "df[features] = scaler.transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_data_path + \"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "ui3xlvplrxcm2eyz613ik"
   },
   "outputs": [],
   "source": [
    "date_to_index = {}\n",
    "dates = list(set(df[\"date\"].values))\n",
    "np.random.shuffle(dates)\n",
    "for i, date in enumerate(dates):\n",
    "    date_to_index[date] = i\n",
    "df[\"order_id\"] = df[\"date\"].apply(lambda x: date_to_index[x])\n",
    "df.sort_values(by=[\"order_id\", \"ts_id\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "p5e4iuqg0kgfaghj5jown8"
   },
   "source": [
    "#### Catboost with random train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6928620\ttest: 0.6930616\tbest: 0.6930616 (0)\ttotal: 122ms\tremaining: 2m 1s\n",
      "250:\tlearn: 0.6451014\ttest: 0.6793427\tbest: 0.6793406 (249)\ttotal: 3.6s\tremaining: 10.7s\n",
      "500:\tlearn: 0.6173234\ttest: 0.6731434\tbest: 0.6731434 (500)\ttotal: 7.01s\tremaining: 6.98s\n",
      "750:\tlearn: 0.5944547\ttest: 0.6679814\tbest: 0.6679814 (750)\ttotal: 10.4s\tremaining: 3.46s\n",
      "999:\tlearn: 0.5756203\ttest: 0.6635319\tbest: 0.6635319 (999)\ttotal: 13.9s\tremaining: 0us\n",
      "bestTest = 0.6635319251\n",
      "bestIteration = 999\n",
      "95950 40883 0.43\n",
      "20083 398 0.02\n",
      "224162 44106 0.2\n",
      "feature_51 :  10.11\n",
      "feature_41 :  5.79\n",
      "feature_43 :  3.13\n",
      "feature_44 :  2.89\n",
      "feature_45 :  2.84\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df[df[\"is_train_1\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if tf.config.list_physical_devices(\"GPU\") else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=train[features],\n",
    "        label=train[\"action\"],\n",
    "        weight=train[\"weight\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=test[features],\n",
    "        label=test[\"action\"],\n",
    "        weight=test[\"weight\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"random split\"] = model\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost with date train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6928000\ttest: 0.6931739\tbest: 0.6931739 (0)\ttotal: 18.4ms\tremaining: 18.4s\n",
      "bestTest = 0.6923080877\n",
      "bestIteration = 32\n",
      "Shrink model to first 33 iterations.\n",
      "95950 14778 0.15\n",
      "20083 278 0.01\n",
      "224162 10909 0.05\n",
      "feature_51 :  10.94\n",
      "feature_41 :  8.54\n",
      "feature_5 :  6.9\n",
      "feature_42 :  5.37\n",
      "feature_39 :  4.59\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if tf.config.list_physical_devices(\"GPU\") else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[train_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[train_idx][\"date\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[test_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[test_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[test_idx][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"group by date split\"] = model\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "101/101 [==============================] - 1s 11ms/step - loss: 0.7144 - AUC: 0.5203 - val_loss: 0.6925 - val_AUC: 0.5270\n",
      "Epoch 2/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6916 - AUC: 0.5325 - val_loss: 0.6931 - val_AUC: 0.5251\n",
      "Epoch 3/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6915 - AUC: 0.5325 - val_loss: 0.6932 - val_AUC: 0.5262\n",
      "Epoch 4/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6912 - AUC: 0.5350 - val_loss: 0.6929 - val_AUC: 0.5251\n",
      "Epoch 5/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6911 - AUC: 0.5362 - val_loss: 0.6929 - val_AUC: 0.5246\n",
      "Epoch 6/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6910 - AUC: 0.5365 - val_loss: 0.6933 - val_AUC: 0.5222\n",
      "Epoch 7/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6910 - AUC: 0.5365 - val_loss: 0.6927 - val_AUC: 0.5249\n",
      "Epoch 8/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6909 - AUC: 0.5375 - val_loss: 0.6930 - val_AUC: 0.5262\n",
      "Epoch 9/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6908 - AUC: 0.5383 - val_loss: 0.6928 - val_AUC: 0.5242\n",
      "Epoch 10/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6908 - AUC: 0.5382 - val_loss: 0.6926 - val_AUC: 0.5261\n",
      "Epoch 11/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6908 - AUC: 0.5382 - val_loss: 0.6928 - val_AUC: 0.5262\n",
      "Epoch 12/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5384 - val_loss: 0.6927 - val_AUC: 0.5264\n",
      "Epoch 13/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6908 - AUC: 0.5381 - val_loss: 0.6927 - val_AUC: 0.5237\n",
      "Epoch 14/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5387 - val_loss: 0.6929 - val_AUC: 0.5243\n",
      "Epoch 15/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5391 - val_loss: 0.6928 - val_AUC: 0.5228\n",
      "Epoch 16/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6906 - AUC: 0.5392 - val_loss: 0.6929 - val_AUC: 0.5243\n",
      "Epoch 17/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5390 - val_loss: 0.6930 - val_AUC: 0.5221\n",
      "Epoch 18/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5382 - val_loss: 0.6929 - val_AUC: 0.5260\n",
      "Epoch 19/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6906 - AUC: 0.5395 - val_loss: 0.6929 - val_AUC: 0.5233\n",
      "Epoch 20/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5384 - val_loss: 0.6928 - val_AUC: 0.5218\n",
      "Epoch 21/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5386 - val_loss: 0.6932 - val_AUC: 0.5247\n",
      "Epoch 22/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5385 - val_loss: 0.6931 - val_AUC: 0.5226\n",
      "Epoch 23/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5387 - val_loss: 0.6932 - val_AUC: 0.5240\n",
      "Epoch 24/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5387 - val_loss: 0.6928 - val_AUC: 0.5269\n",
      "Epoch 25/25\n",
      "101/101 [==============================] - 1s 6ms/step - loss: 0.6907 - AUC: 0.5389 - val_loss: 0.6925 - val_AUC: 0.5264\n",
      "20083 1 0.0\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "inp = L.Input(shape = (len(features),))\n",
    "#x = L.BatchNormalization()(inp)\n",
    "#x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(64)(inp)\n",
    "x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(32)(x)\n",
    "x = L.Dense(1)(x)\n",
    "out = L.Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "    metrics = tf.keras.metrics.AUC(name = \"AUC\")\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "    validation_data=(\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][\"action\"]\n",
    "    ),\n",
    "    epochs=25, \n",
    "    batch_size=8*1024,\n",
    "    callbacks=[],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "tf_models[\"mlp\"] = model\n",
    "K.clear_session()\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [],
   "source": [
    "extended_features = features[:]\n",
    "counter = 1\n",
    "for name, model in catboost_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = model.predict(df[features])\n",
    "    model.save_model(output_data_path + \"catboost_model_\" + str(counter) + \".cbm\")\n",
    "    counter += 1\n",
    "for name, model in tf_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = apply_tf_model(df[features], model)\n",
    "    model.save(output_data_path + \"tf_model_\" + str(counter) + \".h5\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "cellId": "8pqwhx445u8tpd2230as8"
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=2000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=False,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if tf.config.list_physical_devices(\"GPU\") else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_2\"]][extended_features],\n",
    "        label=df[df[\"is_train_2\"]][\"action\"],\n",
    "        weight=df[df[\"is_train_2\"]][\"weight\"],\n",
    "        group_id=df[df[\"is_train_2\"]][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_2\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_train_1\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_val\"]], model, extended_features)\n",
    "estimate_model(df, model, extended_features)\n",
    "feature_importances(model, 5)\n",
    "model.save_model(output_data_path + \"model.cbm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notebookId": "8766bdf2-c6f6-4695-98e2-3af663e5fd96"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
