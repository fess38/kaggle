{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "kviks2l4hc3eyt87l6nfb"
   },
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "okba3s2j2ka81pejlmxh1o"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../input\n",
    "mkdir -p ../output\n",
    "cd ../input\n",
    "\n",
    "export KAGGLE_USERNAME=\"fess38\"\n",
    "export KAGGLE_KEY=\"071966146ec1ebef62023a5efa0574b1\"\n",
    "kaggle competitions download -c jane-street-market-prediction\n",
    "\n",
    "unzip jane-street-market-prediction.zip\n",
    "rm jane-street-market-prediction.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xm2d1ukqreky9p0d2g04fp"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "gyc0u5edjuizta8vsl8hvp",
    "execution_id": "9e442bba-f145-4cea-afdd-a772355bb1b4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "cellId": "o37qnzj77wrdfxqvc6e1jv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import sum_models, CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from catboost.utils import get_gpu_device_count\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "import datatable as dt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "z4g07mdqgheyzvtpn92a5g"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "mpl.rcParams[\"figure.figsize\"] = (11, 5)\n",
    "mpl.rcParams[\"figure.dpi\"]= 100\n",
    "mpl.rcParams[\"lines.linewidth\"] = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2l168l4ogre8w2j8m5wc49"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "tt1yhlytdrwu42ypeukol"
   },
   "outputs": [],
   "source": [
    "input_data_path = \"../input/\"\n",
    "output_data_path = \"../output/\"\n",
    "features = [\"feature_\" + str(i) for i in range(130)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "qywtctpvhzkeomdx3why4o"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "52jqd6zjwdp25uc2uakva2"
   },
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "1skv8wzlynspw13r9lrpn"
   },
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def utility_score(date, weight, resp, action):\n",
    "    pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(pi) / np.sqrt(np.sum(pi**2)) * np.sqrt(250 / len(pi))\n",
    "    return int(min(max(t, 0), 6) * np.sum(pi))\n",
    "\n",
    "def split_df(df, date_splits):\n",
    "    for name, interval in date_splits.items():\n",
    "        df[\"is_\" + name] = df[\"date\"].apply(lambda x: x >= interval[0] and x <= interval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importances(model, top_n=20):\n",
    "    values = sorted(list(zip(model.feature_names_, model.feature_importances_)), key=lambda x: -x[1])\n",
    "    for value in values[:top_n]:\n",
    "        print(value[0], \": \", str(round(value[1], 2)))\n",
    "\n",
    "def estimate_model(df, model, features=features, threshold=0, print_result=True):\n",
    "    expected_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        df[\"action\"].values\n",
    "    )\n",
    "    actual_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        (\n",
    "            (\n",
    "                np.stack(model(df[features].values, training=False).numpy(), axis=1)[0]\n",
    "                if \"tensorflow\" in str(type(model))\n",
    "                else model.predict(df[features], prediction_type=\"RawFormulaVal\")\n",
    "            ) > threshold\n",
    "        ).astype(int)\n",
    "        \n",
    "    )\n",
    "    share = round(actual_score / expected_score, 2)\n",
    "    if print_result:\n",
    "        print(expected_score, actual_score, share)\n",
    "    return actual_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "23qjvnnz2p1sslb8os8379"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "mfficxrfsda57fsipqij7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 s, sys: 18 s, total: 52.4 s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = dt.fread(input_data_path + \"train.csv\").to_pandas()\n",
    "df = df.astype({c: np.float32 for c in df.select_dtypes(include=\"float64\").columns})\n",
    "df[\"action\"] = (df[\"resp\"] > 0).astype(int)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "features_info = pd.read_csv(input_data_path + \"features.csv\")\n",
    "features_info.set_index(keys=[\"feature\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "422axp6sncgt4g88b7x6w"
   },
   "source": [
    "#### Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "qgfdlsoo9h7w61paq1fvn"
   },
   "outputs": [],
   "source": [
    "def fillna_mean(df):\n",
    "    features_mean = df[features].mean()\n",
    "    df[features] = df[features].fillna(features_mean)\n",
    "    with open(output_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "jnjpp98ivlflw88fk4qsj"
   },
   "outputs": [],
   "source": [
    "def fillna_ffill(df):\n",
    "    df[features] = df[features].fillna(method = \"ffill\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "90z64q083eagln57rb5su"
   },
   "outputs": [],
   "source": [
    "def fillna_mean_by_feature_0(df):\n",
    "    features_mean = df[features].groupby(\"feature_0\").mean()\n",
    "    features_mean[\"feature_0\"] = features_mean.index\n",
    "    df.sort_values(by=\"feature_0\", inplace=True)\n",
    "    df[features] = pd.concat([\n",
    "        df[df[\"feature_0\"] == -1][features].fillna(features_mean.loc[-1]),\n",
    "        df[df[\"feature_0\"] == 1][features].fillna(features_mean.loc[1])\n",
    "    ])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    with open(output_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "6bmwkg795e93xd162opb6x"
   },
   "outputs": [],
   "source": [
    "fillna_mean_by_feature_0(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "sipnehvddlc5oea5ywtrvs"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(set(df[\"date\"].values))\n",
    "split = ShuffleSplit(n_splits=5, train_size=0.6, test_size=0.2, random_state=random_state)\n",
    "counter = 1\n",
    "for train_dates, test_dates in split.split(dates):\n",
    "    train_dates = set(train_dates)\n",
    "    test_dates = set(test_dates)\n",
    "    df[\"is_train_\" + str(counter)] = df[\"date\"].apply(lambda x: x in train_dates)\n",
    "    df[\"is_test_\" + str(counter)] = df[\"date\"].apply(lambda x: x in test_dates)\n",
    "    df[\"is_val_\" + str(counter)] = df[\"date\"].apply(lambda x: x not in train_dates and x not in test_dates)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End2End Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = {\n",
    "    \"train\": [0, 449],\n",
    "    \"val\": [450, 499]\n",
    "}\n",
    "split_df(df, date_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = ParameterGrid({\n",
    "    \"iterations\": [2000],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"l2_leaf_reg\": [3],\n",
    "    \"depth\": [16],\n",
    "    \"random_strength\": [1],\n",
    "    \"bagging_temperature\": [1],\n",
    "    \"border_count\": [128],\n",
    "    \"grow_policy\": [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"],\n",
    "    \"use_weight\": [0],\n",
    "    \"use_group_id\": [0]\n",
    "})\n",
    "params_grid = sorted(list(params_grid), key=lambda x: x[\"use_group_id\"])\n",
    "\n",
    "grid_search_result = []\n",
    "dates = list(set(df[df[\"is_train\"]][\"date\"].values))\n",
    "sorted_by_dates, sorted_randomly = False, False\n",
    "\n",
    "for params in tqdm(params_grid, desc=\"Params Tuning\"):\n",
    "    scores = []\n",
    "    if params[\"use_group_id\"] and not sorted_by_dates:\n",
    "        df.sort_values(by=[\"order_id\", \"rnd\"], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        sorted_by_dates = True\n",
    "        sorted_randomly = False\n",
    "    if not params[\"use_group_id\"] and not sorted_randomly:\n",
    "        df.sort_values(by=[\"rnd\"], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        sorted_by_dates = False\n",
    "        sorted_randomly = True\n",
    "\n",
    "    for i in range(3):\n",
    "        train_dates, test_dates = train_test_split(dates, test_size=0.2, random_state=random_state+i)    \n",
    "        model = CatBoostClassifier(\n",
    "            loss_function=\"Logloss\",\n",
    "            iterations=params[\"iterations\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            random_seed=random_state,\n",
    "            l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "            use_best_model=True,\n",
    "            depth=params[\"depth\"],\n",
    "            random_strength=params[\"random_strength\"],\n",
    "            bagging_temperature=params[\"bagging_temperature\"],\n",
    "            border_count=params[\"border_count\"],\n",
    "            grow_policy=params[\"grow_policy\"],\n",
    "            auto_class_weights=\"Balanced\",\n",
    "            early_stopping_rounds=100,\n",
    "            task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X=Pool(\n",
    "                data=df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][features],\n",
    "                label=df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"action\"],\n",
    "                weight=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"weight\"]\n",
    "                    if params[\"use_weight\"] else None,\n",
    "                group_id=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"date\"]\n",
    "                    if params[\"use_group_id\"] else None\n",
    "            ),\n",
    "            eval_set=Pool(\n",
    "                data=df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][features],\n",
    "                label=df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"action\"],\n",
    "                weight=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"weight\"]\n",
    "                    if params[\"use_weight\"] else None,\n",
    "                group_id=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"date\"]\n",
    "                    if params[\"use_group_id\"] else None\n",
    "            )\n",
    "        )\n",
    "        scores.append(estimate_model(df[df[\"is_val\"]], model, print_result=False))\n",
    "        pass\n",
    "    grid_search_result.append({\n",
    "        \"params\": params,\n",
    "        \"score\": sum(scores) / len(scores),\n",
    "        \"best_iteration\": model.best_iteration_,\n",
    "        \"best_score\": model.best_score_\n",
    "    })\n",
    "    grid_search_result = sorted(grid_search_result, key=lambda x: -x[\"score\"])\n",
    "    with open(output_data_path + \"grid_search_result.json\", \"w\") as f:\n",
    "        f.write(json.dumps(grid_search_result, indent=2))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": 60,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"depth\": 12,\n",
    "    \"random_strength\": 1,\n",
    "    \"bagging_temperature\": 1,\n",
    "    \"border_count\": 128,\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "    \"use_weight\": 0,\n",
    "    \"use_group_id\": 1\n",
    "}\n",
    "\n",
    "df.sort_values(\n",
    "    by=[\"order_id\", \"rnd\"] if params[\"use_group_id\"] else [\"rnd\"],\n",
    "    inplace=True\n",
    ")\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    iterations=params[\"iterations\"],\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "    depth=params[\"depth\"],\n",
    "    random_strength=params[\"random_strength\"],\n",
    "    bagging_temperature=params[\"bagging_temperature\"],\n",
    "    border_count=params[\"border_count\"],\n",
    "    grow_policy=params[\"grow_policy\"],\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[features],\n",
    "        label=df[\"action\"],\n",
    "        weight=df[\"weight\"] if params[\"use_weight\"] else None,\n",
    "        group_id=df[\"date\"] if params[\"use_group_id\"] else None\n",
    "    )\n",
    ")\n",
    "\n",
    "estimate_model(df[df[\"is_train\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "model.save_model(output_data_path + \"model.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_data_path + \"grid_search_result.json\", \"r\") as f:\n",
    "    grid_search_results = json.loads(f.read())\n",
    "data = {}\n",
    "param = \"use_group_id\"\n",
    "for grid_search_result in grid_search_results:\n",
    "    params = grid_search_result[\"params\"]\n",
    "    data[params[param]] = data.get(params[param], []) + [grid_search_result[\"score\"]]\n",
    "for key, values in data.items():\n",
    "    values = np.array(values)\n",
    "    print(key, int(np.mean(values)), int(np.median(values)), int(max(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df[features])\n",
    "df[features] = scaler.transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(encoding_dim):    \n",
    "    def apply_bn_and_dropout(x):\n",
    "        return L.Dropout(0.2)(L.BatchNormalization()(x))\n",
    "    \n",
    "    inp = L.Input(len(features))\n",
    "    x = L.BatchNormalization()(inp)\n",
    "    x = L.GaussianNoise(0.1)(x)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    encoded = L.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    \n",
    "    input_encoded = L.Input(encoding_dim)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(input_encoded)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    decoded = L.Dense(len(features), activation=\"linear\")(x)\n",
    "\n",
    "    encoder = Model(inp, encoded, name=\"encoder_\" + str(encoding_dim))\n",
    "    decoder = Model(input_encoded, decoded, name=\"decoder\")\n",
    "    autoencoder = Model(inp, decoder(encoder(inp)), name=\"autoencoder\")\n",
    "    return encoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = []\n",
    "K.backend.clear_session()\n",
    "for encode_dim in [96, 64, 32]: \n",
    "    encoder, autoencoder = create_autoencoder(encode_dim)\n",
    "    autoencoder.compile(optimizer=K.optimizers.RMSprop(1e-4), loss=\"mse\")\n",
    "    \n",
    "    autoencoder.fit(\n",
    "        df[features],\n",
    "        df[features],\n",
    "        batch_size=1024*4,\n",
    "        epochs=100,\n",
    "        verbose=0,\n",
    "        callbacks=[K.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        validation_split=0.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "    encoder.save(output_data_path + \"encoder_{}.h5\".format(encode_dim))\n",
    "    encoders.append(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = encoder(df[features].values, training=False).numpy()\n",
    "size = len(new_features[0])\n",
    "new_columns = [\"enc_features_{}_{}\".format(size, i) for i in range(size)]\n",
    "df[new_columns] = pd.DataFrame(new_features, index=df.index)\n",
    "extended_features = features[:] + new_columns\n",
    "del new_features, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [\n",
    "    K.models.load_model(output_data_path + \"encoder_96.h5\"),\n",
    "    K.models.load_model(output_data_path + \"encoder_64.h5\"),\n",
    "    K.models.load_model(output_data_path + \"encoder_32.h5\")\n",
    "]\n",
    "for encoder in encoders:\n",
    "    encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(encoders):\n",
    "    def apply_bn_and_dropout(x):\n",
    "        return L.Dropout(0.2)(L.BatchNormalization()(x))\n",
    "    \n",
    "    inp = L.Input(len(features))\n",
    "    x = L.Concatenate()( [inp] + [encoder(inp) for encoder in encoders])\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dense(128, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(128, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(128, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(1)(x)\n",
    "    output = L.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=output, name=\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 130)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_96 (Functional)         (None, 96)           32488       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_64 (Functional)         (None, 64)           17736       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_32 (Functional)         (None, 32)           7080        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 322)          0           input_1[0][0]                    \n",
      "                                                                 encoder_96[1][0]                 \n",
      "                                                                 encoder_64[1][0]                 \n",
      "                                                                 encoder_32[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 322)          1288        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          41344       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 134,625\n",
      "Trainable params: 75,909\n",
      "Non-trainable params: 58,716\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.backend.clear_session()\n",
    "model = mlp(encoders)\n",
    "model.compile(\n",
    "    optimizer=K.optimizers.Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "175/175 [==============================] - 3s 17ms/step - loss: 0.7762 - precision: 0.5064 - recall: 0.5045 - val_loss: 0.6977 - val_precision: 0.5161 - val_recall: 0.4558\n",
      "Epoch 2/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7338 - precision: 0.5085 - recall: 0.5025 - val_loss: 0.6954 - val_precision: 0.5158 - val_recall: 0.5240\n",
      "Epoch 3/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7219 - precision: 0.5090 - recall: 0.5036 - val_loss: 0.6938 - val_precision: 0.5161 - val_recall: 0.5504\n",
      "Epoch 4/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7149 - precision: 0.5101 - recall: 0.5034 - val_loss: 0.6932 - val_precision: 0.5209 - val_recall: 0.4751\n",
      "Epoch 5/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7110 - precision: 0.5109 - recall: 0.5044 - val_loss: 0.6927 - val_precision: 0.5212 - val_recall: 0.4865\n",
      "Epoch 6/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7079 - precision: 0.5118 - recall: 0.5040 - val_loss: 0.6922 - val_precision: 0.5217 - val_recall: 0.4981\n",
      "Epoch 7/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7052 - precision: 0.5126 - recall: 0.5054 - val_loss: 0.6920 - val_precision: 0.5231 - val_recall: 0.4871\n",
      "Epoch 8/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7035 - precision: 0.5138 - recall: 0.5090 - val_loss: 0.6919 - val_precision: 0.5224 - val_recall: 0.5077\n",
      "Epoch 9/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7019 - precision: 0.5143 - recall: 0.5071 - val_loss: 0.6918 - val_precision: 0.5246 - val_recall: 0.4830\n",
      "Epoch 10/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.7006 - precision: 0.5155 - recall: 0.5078 - val_loss: 0.6922 - val_precision: 0.5275 - val_recall: 0.4455\n",
      "Epoch 11/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6998 - precision: 0.5166 - recall: 0.5100 - val_loss: 0.6918 - val_precision: 0.5243 - val_recall: 0.5110\n",
      "Epoch 12/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6986 - precision: 0.5176 - recall: 0.5174 - val_loss: 0.6920 - val_precision: 0.5261 - val_recall: 0.4901\n",
      "Epoch 13/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6980 - precision: 0.5182 - recall: 0.5134 - val_loss: 0.6918 - val_precision: 0.5249 - val_recall: 0.5150\n",
      "Epoch 14/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6974 - precision: 0.5191 - recall: 0.5125 - val_loss: 0.6918 - val_precision: 0.5260 - val_recall: 0.5133\n",
      "Epoch 15/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6967 - precision: 0.5203 - recall: 0.5158 - val_loss: 0.6919 - val_precision: 0.5263 - val_recall: 0.5038\n",
      "Epoch 16/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6964 - precision: 0.5203 - recall: 0.5182 - val_loss: 0.6918 - val_precision: 0.5271 - val_recall: 0.4883\n",
      "Epoch 17/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6956 - precision: 0.5212 - recall: 0.5150 - val_loss: 0.6920 - val_precision: 0.5293 - val_recall: 0.4559\n",
      "Epoch 18/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6951 - precision: 0.5222 - recall: 0.5166 - val_loss: 0.6917 - val_precision: 0.5253 - val_recall: 0.5180\n",
      "Epoch 19/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6949 - precision: 0.5225 - recall: 0.5207 - val_loss: 0.6917 - val_precision: 0.5265 - val_recall: 0.4982\n",
      "Epoch 20/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6944 - precision: 0.5225 - recall: 0.5134 - val_loss: 0.6917 - val_precision: 0.5279 - val_recall: 0.4801\n",
      "Epoch 21/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6944 - precision: 0.5233 - recall: 0.5103 - val_loss: 0.6917 - val_precision: 0.5277 - val_recall: 0.4849\n",
      "Epoch 22/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6939 - precision: 0.5238 - recall: 0.5189 - val_loss: 0.6916 - val_precision: 0.5273 - val_recall: 0.4952\n",
      "Epoch 23/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6934 - precision: 0.5243 - recall: 0.5084 - val_loss: 0.6917 - val_precision: 0.5285 - val_recall: 0.4763\n",
      "Epoch 24/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6933 - precision: 0.5247 - recall: 0.5120 - val_loss: 0.6916 - val_precision: 0.5271 - val_recall: 0.5018\n",
      "Epoch 25/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6931 - precision: 0.5238 - recall: 0.5181 - val_loss: 0.6916 - val_precision: 0.5281 - val_recall: 0.4858\n",
      "Epoch 26/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6927 - precision: 0.5252 - recall: 0.5157 - val_loss: 0.6915 - val_precision: 0.5283 - val_recall: 0.4842\n",
      "Epoch 27/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6927 - precision: 0.5262 - recall: 0.5104 - val_loss: 0.6917 - val_precision: 0.5276 - val_recall: 0.4906\n",
      "Epoch 28/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6924 - precision: 0.5261 - recall: 0.5050 - val_loss: 0.6916 - val_precision: 0.5279 - val_recall: 0.4924\n",
      "Epoch 29/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6921 - precision: 0.5265 - recall: 0.5147 - val_loss: 0.6916 - val_precision: 0.5275 - val_recall: 0.5013\n",
      "Epoch 30/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6920 - precision: 0.5271 - recall: 0.5061 - val_loss: 0.6917 - val_precision: 0.5293 - val_recall: 0.4741\n",
      "Epoch 31/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6919 - precision: 0.5271 - recall: 0.5104 - val_loss: 0.6917 - val_precision: 0.5290 - val_recall: 0.4801\n",
      "Epoch 32/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6917 - precision: 0.5278 - recall: 0.5125 - val_loss: 0.6915 - val_precision: 0.5278 - val_recall: 0.4939\n",
      "Epoch 33/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6915 - precision: 0.5287 - recall: 0.5083 - val_loss: 0.6916 - val_precision: 0.5289 - val_recall: 0.4766\n",
      "Epoch 34/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6915 - precision: 0.5284 - recall: 0.5096 - val_loss: 0.6915 - val_precision: 0.5286 - val_recall: 0.4834\n",
      "Epoch 35/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6913 - precision: 0.5284 - recall: 0.5033 - val_loss: 0.6915 - val_precision: 0.5278 - val_recall: 0.5014\n",
      "Epoch 36/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6912 - precision: 0.5288 - recall: 0.5082 - val_loss: 0.6915 - val_precision: 0.5296 - val_recall: 0.4807\n",
      "Epoch 37/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6910 - precision: 0.5298 - recall: 0.5041 - val_loss: 0.6915 - val_precision: 0.5285 - val_recall: 0.4799\n",
      "Epoch 38/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6908 - precision: 0.5296 - recall: 0.5090 - val_loss: 0.6915 - val_precision: 0.5287 - val_recall: 0.4867\n",
      "Epoch 39/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6907 - precision: 0.5293 - recall: 0.5157 - val_loss: 0.6915 - val_precision: 0.5288 - val_recall: 0.4812\n",
      "Epoch 40/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6906 - precision: 0.5305 - recall: 0.5048 - val_loss: 0.6916 - val_precision: 0.5292 - val_recall: 0.4798\n",
      "Epoch 41/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6905 - precision: 0.5298 - recall: 0.5119 - val_loss: 0.6916 - val_precision: 0.5283 - val_recall: 0.4865\n",
      "Epoch 42/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6904 - precision: 0.5310 - recall: 0.5064 - val_loss: 0.6916 - val_precision: 0.5296 - val_recall: 0.4733\n",
      "Epoch 43/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6902 - precision: 0.5315 - recall: 0.5065 - val_loss: 0.6916 - val_precision: 0.5275 - val_recall: 0.4957\n",
      "Epoch 44/100\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.6901 - precision: 0.5311 - recall: 0.5125 - val_loss: 0.6917 - val_precision: 0.5277 - val_recall: 0.4890\n",
      "Epoch 45/100\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.6901 - precision: 0.5312 - recall: 0.5101 - val_loss: 0.6915 - val_precision: 0.5275 - val_recall: 0.4957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe2b3eea1d0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = class_weight.compute_class_weight(\n",
    "    \"balanced\",\n",
    "    np.unique(df[df[\"is_train_1\"]][\"action\"]),\n",
    "    df[df[\"is_train_1\"]][\"action\"]\n",
    ")\n",
    "model.fit(\n",
    "    df[df[\"is_train_1\"]][features],\n",
    "    df[df[\"is_train_1\"]][\"action\"],\n",
    "    batch_size=1024*8,\n",
    "    epochs=100,\n",
    "    callbacks=[K.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "    validation_data=(\n",
    "        df[df[\"is_test_1\"]][features],\n",
    "        df[df[\"is_test_1\"]][\"action\"]\n",
    "    ),\n",
    "    class_weight={\n",
    "        0: weights[0],\n",
    "        1: weights[1]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46303 2096 0.05\n",
      "47788 1535 0.03\n",
      "40814 1142 0.03\n",
      "42194 1776 0.04\n",
      "45483 903 0.02\n",
      "224162 13248 0.06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13248"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(df[df[\"is_val_1\"]], model, threshold=0.50)\n",
    "estimate_model(df[df[\"is_val_2\"]], model, threshold=0.50)\n",
    "estimate_model(df[df[\"is_val_3\"]], model, threshold=0.50)\n",
    "estimate_model(df[df[\"is_val_4\"]], model, threshold=0.50)\n",
    "estimate_model(df[df[\"is_val_5\"]], model, threshold=0.50)\n",
    "estimate_model(df, model, threshold=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(output_data_path + \"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[df[\"is_train\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=pd.concat([train[features].reset_index(drop=True), pd.DataFrame(encoder(train[features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        label=train[\"action\"].values,\n",
    "        weight=train[\"weight\"].values\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=pd.concat([test[features].reset_index(drop=True), pd.DataFrame(encoder(test[features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        label=test[\"action\"].values,\n",
    "        weight=test[\"weight\"].values\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(\n",
    "    df[df[\"is_val\"]][\"date\"].values,\n",
    "    df[df[\"is_val\"]][\"weight\"].values,\n",
    "    df[df[\"is_val\"]][\"resp\"].values,\n",
    "    (model.predict(\n",
    "        pd.concat([df[df[\"is_val\"]][features].reset_index(drop=True), pd.DataFrame(encoder(df[df[\"is_val\"]][features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        prediction_type=\"RawFormulaVal\") > 0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = L.Input(130)\n",
    "encoded = L.BatchNormalization()(i)\n",
    "encoded = L.GaussianNoise(0.1)(encoded)\n",
    "encoded = L.Dense(64,activation='relu')(encoded)\n",
    "decoded = L.Dropout(0.2)(encoded)\n",
    "decoded = L.Dense(130, name='decoded')(decoded)\n",
    "x = L.Dense(64,activation='relu')(decoded)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(64,activation='relu')(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dropout(0.2)(x)    \n",
    "x = L.Dense(1, activation='sigmoid', name='label_output')(x)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n",
    "autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n",
    "\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.0001),loss={'decoded':'mse', 'label_output':'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(\n",
    "    df[df[\"is_train\"]][features],\n",
    "    (df[df[\"is_train\"]][features], df[df[\"is_train\"]][\"action\"]),\n",
    "    epochs=25,\n",
    "    batch_size=4096, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping('val_loss', patience=10,restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.3).astype(int),\n",
    "    df[df[\"is_val\"]][\"action\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.3).astype(int),\n",
    "    df[df[\"is_val\"]][\"action\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(\n",
    "    df[df[\"is_val\"]][\"date\"].values,\n",
    "    df[df[\"is_val\"]][\"weight\"].values,\n",
    "    df[df[\"is_val\"]][\"resp\"].values,\n",
    "    #df[df[\"is_val\"]][\"action\"].values\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"is_val\"]].query(\"weight > 3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-stage model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to 2-stage train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = {\n",
    "    \"train_1\": [0, 224],\n",
    "    \"train_2\": [225, 449],\n",
    "    \"val\": [450, 499]\n",
    "}\n",
    "split_df(df, date_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df[(df[\"is_train_1\"])|(df[\"is_train_2\"])][features])\n",
    "df[features] = scaler.transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_data_path + \"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "p5e4iuqg0kgfaghj5jown8"
   },
   "source": [
    "#### Catboost with random train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[df[\"is_train_1\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=train[features],\n",
    "        label=train[\"action\"],\n",
    "        weight=train[\"weight\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=test[features],\n",
    "        label=test[\"action\"],\n",
    "        weight=test[\"weight\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"random split\"] = model\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost with date train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[train_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[train_idx][\"date\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[test_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[test_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[test_idx][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"group by date split\"] = model\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "inp = L.Input(shape = (len(features),))\n",
    "#x = L.BatchNormalization()(inp)\n",
    "#x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(64)(inp)\n",
    "x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(32)(x)\n",
    "x = L.Dense(1)(x)\n",
    "out = L.Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "    metrics = tf.keras.metrics.AUC(name = \"AUC\")\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "    validation_data=(\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][\"action\"]\n",
    "    ),\n",
    "    epochs=1000, \n",
    "    batch_size=8*1024,\n",
    "    callbacks=[],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "tf_models[\"mlp\"] = model\n",
    "K.backend.clear_session()\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [],
   "source": [
    "extended_features = features[:]\n",
    "counter = 1\n",
    "for name, model in catboost_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = model.predict(df[features])\n",
    "    model.save_model(output_data_path + \"catboost_model_\" + str(counter) + \".cbm\")\n",
    "    counter += 1\n",
    "for name, model in tf_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = apply_tf_model(df[features], model)\n",
    "    model.save(output_data_path + \"tf_model_\" + str(counter) + \".h5\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8pqwhx445u8tpd2230as8"
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=2000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=False,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_2\"]][extended_features],\n",
    "        label=df[df[\"is_train_2\"]][\"action\"],\n",
    "        weight=df[df[\"is_train_2\"]][\"weight\"],\n",
    "        group_id=df[df[\"is_train_2\"]][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_2\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_train_1\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_val\"]], model, extended_features)\n",
    "estimate_model(df, model, extended_features)\n",
    "feature_importances(model, 5)\n",
    "model.save_model(output_data_path + \"model.cbm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notebookId": "8766bdf2-c6f6-4695-98e2-3af663e5fd96",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
