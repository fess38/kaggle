{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "kviks2l4hc3eyt87l6nfb"
   },
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "okba3s2j2ka81pejlmxh1o"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../input\n",
    "mkdir -p ../output\n",
    "cd ../input\n",
    "\n",
    "export KAGGLE_USERNAME=\"fess38\"\n",
    "export KAGGLE_KEY=\"071966146ec1ebef62023a5efa0574b1\"\n",
    "kaggle competitions download -c jane-street-market-prediction\n",
    "\n",
    "unzip jane-street-market-prediction.zip\n",
    "rm jane-street-market-prediction.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xm2d1ukqreky9p0d2g04fp"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "gyc0u5edjuizta8vsl8hvp",
    "execution_id": "9e442bba-f145-4cea-afdd-a772355bb1b4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "o37qnzj77wrdfxqvc6e1jv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import sum_models, CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from catboost.utils import get_gpu_device_count\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "z4g07mdqgheyzvtpn92a5g"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "mpl.rcParams[\"figure.figsize\"] = (11, 5)\n",
    "mpl.rcParams[\"figure.dpi\"]= 100\n",
    "mpl.rcParams[\"lines.linewidth\"] = 0.75\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2l168l4ogre8w2j8m5wc49"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "tt1yhlytdrwu42ypeukol"
   },
   "outputs": [],
   "source": [
    "input_data_path = \"../input/\"\n",
    "output_data_path = \"../output/\"\n",
    "features = [\"feature_\" + str(i) for i in range(130)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "qywtctpvhzkeomdx3why4o"
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "52jqd6zjwdp25uc2uakva2"
   },
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "1skv8wzlynspw13r9lrpn"
   },
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def utility_score(date, weight, resp, action):\n",
    "    pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(pi) / np.sqrt(np.sum(pi**2)) * np.sqrt(250 / len(pi))\n",
    "    return int(min(max(t, 0), 6) * np.sum(pi))\n",
    "\n",
    "def split_df(df, date_splits):\n",
    "    for name, interval in date_splits.items():\n",
    "        df[\"is_\" + name] = df[\"date\"].apply(lambda x: x >= interval[0] and x <= interval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importances(model, top_n=20):\n",
    "    values = sorted(list(zip(model.feature_names_, model.feature_importances_)), key=lambda x: -x[1])\n",
    "    for value in values[:top_n]:\n",
    "        print(value[0], \": \", str(round(value[1], 2)))\n",
    "\n",
    "def estimate_model(df, model, features=features, threshold=0, print_result=True):\n",
    "    expected_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        df[\"action\"].values\n",
    "    )\n",
    "    actual_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        np.stack(model(df[features].values, training=False).numpy(), axis=1)[0]\n",
    "        if \"tensorflow\" in str(type(model))\n",
    "        else(model.predict(df[features], prediction_type=\"RawFormulaVal\") > threshold).astype(int)\n",
    "    )\n",
    "    share = round(actual_score / expected_score, 2)\n",
    "    if print_result:\n",
    "        print(expected_score, actual_score, share)\n",
    "    return actual_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tf_model(df, model):\n",
    "    return model(df.values, training=False).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "23qjvnnz2p1sslb8os8379"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "mfficxrfsda57fsipqij7n"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_data_path + \"train.csv\")\n",
    "df = df.astype({c: np.float32 for c in df.select_dtypes(include=\"float64\").columns})\n",
    "\n",
    "features_info = pd.read_csv(input_data_path + \"features.csv\")\n",
    "features_info.set_index(keys=[\"feature\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action\"] = (df[\"resp\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "422axp6sncgt4g88b7x6w"
   },
   "source": [
    "#### Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "qgfdlsoo9h7w61paq1fvn"
   },
   "outputs": [],
   "source": [
    "def fillna_mean(df):\n",
    "    features_mean = df[features].mean()\n",
    "    df[features] = df[features].fillna(features_mean)\n",
    "    with open(output_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "jnjpp98ivlflw88fk4qsj"
   },
   "outputs": [],
   "source": [
    "def fillna_ffill(df):\n",
    "    df[features] = df[features].fillna(method = \"ffill\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "90z64q083eagln57rb5su"
   },
   "outputs": [],
   "source": [
    "def fillna_mean_by_feature_0(df):\n",
    "    features_mean = df[features].groupby(\"feature_0\").mean()\n",
    "    features_mean[\"feature_0\"] = features_mean.index\n",
    "    df.sort_values(by=\"feature_0\", inplace=True)\n",
    "    df[features] = pd.concat([\n",
    "        df[df[\"feature_0\"] == -1][features].fillna(features_mean.loc[-1]),\n",
    "        df[df[\"feature_0\"] == 1][features].fillna(features_mean.loc[1])\n",
    "    ])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    with open(output_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "6bmwkg795e93xd162opb6x"
   },
   "outputs": [],
   "source": [
    "fillna_mean_by_feature_0(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "sipnehvddlc5oea5ywtrvs"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rnd\"] = np.random.rand(len(df))\n",
    "date_to_index = {}\n",
    "dates = list(set(df[\"date\"].values))\n",
    "np.random.shuffle(dates)\n",
    "for i, date in enumerate(dates):\n",
    "    date_to_index[date] = i\n",
    "df[\"order_id\"] = df[\"date\"].apply(lambda x: date_to_index[x])\n",
    "df.sort_values(by=[\"order_id\", \"rnd\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End2End Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = {\n",
    "    \"train\": [0, 449],\n",
    "    \"val\": [450, 499]\n",
    "}\n",
    "split_df(df, date_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = ParameterGrid({\n",
    "    \"iterations\": [2000],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"l2_leaf_reg\": [3],\n",
    "    \"depth\": [16],\n",
    "    \"random_strength\": [1],\n",
    "    \"bagging_temperature\": [1],\n",
    "    \"border_count\": [128],\n",
    "    \"grow_policy\": [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"],\n",
    "    \"use_weight\": [0],\n",
    "    \"use_group_id\": [0]\n",
    "})\n",
    "params_grid = sorted(list(params_grid), key=lambda x: x[\"use_group_id\"])\n",
    "\n",
    "grid_search_result = []\n",
    "dates = list(set(df[df[\"is_train\"]][\"date\"].values))\n",
    "sorted_by_dates, sorted_randomly = False, False\n",
    "\n",
    "for params in tqdm(params_grid, desc=\"Params Tuning\"):\n",
    "    scores = []\n",
    "    if params[\"use_group_id\"] and not sorted_by_dates:\n",
    "        df.sort_values(by=[\"order_id\", \"rnd\"], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        sorted_by_dates = True\n",
    "        sorted_randomly = False\n",
    "    if not params[\"use_group_id\"] and not sorted_randomly:\n",
    "        df.sort_values(by=[\"rnd\"], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        sorted_by_dates = False\n",
    "        sorted_randomly = True\n",
    "\n",
    "    for i in range(3):\n",
    "        train_dates, test_dates = train_test_split(dates, test_size=0.2, random_state=random_state+i)    \n",
    "        model = CatBoostClassifier(\n",
    "            loss_function=\"Logloss\",\n",
    "            iterations=params[\"iterations\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            random_seed=random_state,\n",
    "            l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "            use_best_model=True,\n",
    "            depth=params[\"depth\"],\n",
    "            random_strength=params[\"random_strength\"],\n",
    "            bagging_temperature=params[\"bagging_temperature\"],\n",
    "            border_count=params[\"border_count\"],\n",
    "            grow_policy=params[\"grow_policy\"],\n",
    "            auto_class_weights=\"Balanced\",\n",
    "            early_stopping_rounds=100,\n",
    "            task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X=Pool(\n",
    "                data=df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][features],\n",
    "                label=df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"action\"],\n",
    "                weight=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"weight\"]\n",
    "                    if params[\"use_weight\"] else None,\n",
    "                group_id=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"date\"]\n",
    "                    if params[\"use_group_id\"] else None\n",
    "            ),\n",
    "            eval_set=Pool(\n",
    "                data=df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][features],\n",
    "                label=df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"action\"],\n",
    "                weight=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"weight\"]\n",
    "                    if params[\"use_weight\"] else None,\n",
    "                group_id=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"date\"]\n",
    "                    if params[\"use_group_id\"] else None\n",
    "            )\n",
    "        )\n",
    "        scores.append(estimate_model(df[df[\"is_val\"]], model, print_result=False))\n",
    "        pass\n",
    "    grid_search_result.append({\n",
    "        \"params\": params,\n",
    "        \"score\": sum(scores) / len(scores),\n",
    "        \"best_iteration\": model.best_iteration_,\n",
    "        \"best_score\": model.best_score_\n",
    "    })\n",
    "    grid_search_result = sorted(grid_search_result, key=lambda x: -x[\"score\"])\n",
    "    with open(output_data_path + \"grid_search_result.json\", \"w\") as f:\n",
    "        f.write(json.dumps(grid_search_result, indent=2))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": 60,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"depth\": 12,\n",
    "    \"random_strength\": 1,\n",
    "    \"bagging_temperature\": 1,\n",
    "    \"border_count\": 128,\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "    \"use_weight\": 0,\n",
    "    \"use_group_id\": 1\n",
    "}\n",
    "\n",
    "df.sort_values(\n",
    "    by=[\"order_id\", \"rnd\"] if params[\"use_group_id\"] else [\"rnd\"],\n",
    "    inplace=True\n",
    ")\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    iterations=params[\"iterations\"],\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "    depth=params[\"depth\"],\n",
    "    random_strength=params[\"random_strength\"],\n",
    "    bagging_temperature=params[\"bagging_temperature\"],\n",
    "    border_count=params[\"border_count\"],\n",
    "    grow_policy=params[\"grow_policy\"],\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[features],\n",
    "        label=df[\"action\"],\n",
    "        weight=df[\"weight\"] if params[\"use_weight\"] else None,\n",
    "        group_id=df[\"date\"] if params[\"use_group_id\"] else None\n",
    "    )\n",
    ")\n",
    "\n",
    "estimate_model(df[df[\"is_train\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "model.save_model(output_data_path + \"model.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_data_path + \"grid_search_result.json\", \"r\") as f:\n",
    "    grid_search_results = json.loads(f.read())\n",
    "data = {}\n",
    "param = \"use_group_id\"\n",
    "for grid_search_result in grid_search_results:\n",
    "    params = grid_search_result[\"params\"]\n",
    "    data[params[param]] = data.get(params[param], []) + [grid_search_result[\"score\"]]\n",
    "for key, values in data.items():\n",
    "    values = np.array(values)\n",
    "    print(key, int(np.mean(values)), int(np.median(values)), int(max(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df[df[\"is_train\"]][features])\n",
    "df[features] = scaler.transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(encoding_dim):    \n",
    "    def apply_bn_and_dropout(x):\n",
    "        return L.Dropout(0.2)(L.BatchNormalization()(x))\n",
    "    \n",
    "    inp = L.Input(len(features))\n",
    "    x = L.GaussianNoise(0.1)(inp)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    encoded = L.Dense(encoding_dim, activation=\"linear\")(x)\n",
    "    \n",
    "    input_encoded = L.Input(encoding_dim)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(input_encoded)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    decoded = L.Dense(len(features), activation=\"sigmoid\")(x)\n",
    "\n",
    "    encoder = Model(inp, encoded, name=\"encoder\")\n",
    "    decoder = Model(input_encoded, decoded, name=\"decoder\")\n",
    "    autoencoder = Model(inp, decoder(encoder(inp)), name=\"autoencoder\")\n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 130)]             0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 64)                17216     \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 130)               17282     \n",
      "=================================================================\n",
      "Total params: 34,498\n",
      "Trainable params: 33,986\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.backend.clear_session()\n",
    "encoder, decoder, autoencoder = create_autoencoder(64)\n",
    "autoencoder.compile(optimizer=Adam(1e-4), loss=\"binary_crossentropy\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6601/6601 [==============================] - 29s 4ms/step - loss: 0.4643 - val_loss: 0.4087\n",
      "Epoch 2/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4102 - val_loss: 0.4074\n",
      "Epoch 3/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4087 - val_loss: 0.4070\n",
      "Epoch 4/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4084 - val_loss: 0.4068\n",
      "Epoch 5/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4082 - val_loss: 0.4068\n",
      "Epoch 6/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4081 - val_loss: 0.4067\n",
      "Epoch 7/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4080 - val_loss: 0.4066\n",
      "Epoch 8/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4079 - val_loss: 0.4066\n",
      "Epoch 9/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4078 - val_loss: 0.4065\n",
      "Epoch 10/10\n",
      "6601/6601 [==============================] - 27s 4ms/step - loss: 0.4078 - val_loss: 0.4065\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df[df[\"is_train\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "autoencoder.fit(\n",
    "    train[features], train[features],\n",
    "    epochs=10,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_data=(test[features], test[features])\n",
    ")\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(\n",
    "    df[df[\"is_val\"]][features][:10],\n",
    "    autoencoder.predict(df[df[\"is_val\"]][features][:10])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save(output_data_path + \"encoder_64.h5\")\n",
    "decoder.save(output_data_path + \"decoder_64.h5\")\n",
    "autoencoder.save(output_data_path + \"autoencoder_64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = encoder(df[features].values, training=False).numpy()\n",
    "size = len(new_features[0])\n",
    "new_columns = [\"enc_features_{}_{}\".format(size, i) for i in range(size)]\n",
    "df[new_columns] = pd.DataFrame(new_features, index=df.index)\n",
    "extended_features = features[:] + new_columns\n",
    "del new_features, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(encoder):\n",
    "    def apply_bn_and_dropout(x):\n",
    "        return L.Dropout(0.2)(L.BatchNormalization()(x))\n",
    "    \n",
    "    inp = L.Input(len(features))\n",
    "    x = L.Concatenate()([inp, encoder(inp)])\n",
    "    x = L.Dense(256, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(256, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(256, activation=\"relu\")(x)\n",
    "    x = apply_bn_and_dropout(x)\n",
    "    x = L.Dense(1)(x)\n",
    "    output = L.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 130)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, 64)           17216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 194)          0           input_1[0][0]                    \n",
      "                                                                 encoder[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          49920       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            257         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 202,049\n",
      "Trainable params: 183,297\n",
      "Non-trainable params: 18,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.backend.clear_session()\n",
    "encoder.trainable = False\n",
    "model = mlp(encoder)\n",
    "model.compile(\n",
    "    optimizer=Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=tf.keras.metrics.AUC(name = \"AUC\")\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[df[\"is_train\"]], test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6601/6601 [==============================] - 32s 5ms/step - loss: 0.6968 - AUC: 0.5116 - val_loss: 0.6946 - val_AUC: 0.5232\n",
      "Epoch 2/10\n",
      "6601/6601 [==============================] - 32s 5ms/step - loss: 0.6930 - AUC: 0.5165 - val_loss: 0.6922 - val_AUC: 0.5212\n",
      "Epoch 3/10\n",
      "5404/6601 [=======================>......] - ETA: 5s - loss: 0.6921 - AUC: 0.5241"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-f7ccaffe38fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     validation_data=(\n\u001b[1;32m      7\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train[features], train[\"action\"],\n",
    "    epochs=10,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_data=(\n",
    "        test[features],\n",
    "        test[\"action\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "#del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20083 11 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model(df[df[\"is_val\"]], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[df[\"is_train\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=pd.concat([train[features].reset_index(drop=True), pd.DataFrame(encoder(train[features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        label=train[\"action\"].values,\n",
    "        weight=train[\"weight\"].values\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=pd.concat([test[features].reset_index(drop=True), pd.DataFrame(encoder(test[features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        label=test[\"action\"].values,\n",
    "        weight=test[\"weight\"].values\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(\n",
    "    df[df[\"is_val\"]][\"date\"].values,\n",
    "    df[df[\"is_val\"]][\"weight\"].values,\n",
    "    df[df[\"is_val\"]][\"resp\"].values,\n",
    "    (model.predict(\n",
    "        pd.concat([df[df[\"is_val\"]][features].reset_index(drop=True), pd.DataFrame(encoder(df[df[\"is_val\"]][features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        prediction_type=\"RawFormulaVal\") > 0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = L.Input(130)\n",
    "encoded = L.BatchNormalization()(i)\n",
    "encoded = L.GaussianNoise(0.1)(encoded)\n",
    "encoded = L.Dense(64,activation='relu')(encoded)\n",
    "decoded = L.Dropout(0.2)(encoded)\n",
    "decoded = L.Dense(130, name='decoded')(decoded)\n",
    "x = L.Dense(64,activation='relu')(decoded)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(64,activation='relu')(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dropout(0.2)(x)    \n",
    "x = L.Dense(1, activation='sigmoid', name='label_output')(x)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n",
    "autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n",
    "\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.0001),loss={'decoded':'mse', 'label_output':'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(\n",
    "    df[df[\"is_train\"]][features],\n",
    "    (df[df[\"is_train\"]][features], df[df[\"is_train\"]][\"action\"]),\n",
    "    epochs=25,\n",
    "    batch_size=4096, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping('val_loss', patience=10,restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.3).astype(int),\n",
    "    df[df[\"is_val\"]][\"action\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.3).astype(int),\n",
    "    df[df[\"is_val\"]][\"action\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(\n",
    "    df[df[\"is_val\"]][\"date\"].values,\n",
    "    df[df[\"is_val\"]][\"weight\"].values,\n",
    "    df[df[\"is_val\"]][\"resp\"].values,\n",
    "    #df[df[\"is_val\"]][\"action\"].values\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"is_val\"]].query(\"weight > 3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-stage model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to 2-stage train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = {\n",
    "    \"train_1\": [0, 224],\n",
    "    \"train_2\": [225, 449],\n",
    "    \"val\": [450, 499]\n",
    "}\n",
    "split_df(df, date_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df[(df[\"is_train_1\"])|(df[\"is_train_2\"])][features])\n",
    "df[features] = scaler.transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_data_path + \"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "p5e4iuqg0kgfaghj5jown8"
   },
   "source": [
    "#### Catboost with random train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[df[\"is_train_1\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=train[features],\n",
    "        label=train[\"action\"],\n",
    "        weight=train[\"weight\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=test[features],\n",
    "        label=test[\"action\"],\n",
    "        weight=test[\"weight\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"random split\"] = model\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost with date train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[train_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[train_idx][\"date\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[test_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[test_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[test_idx][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"group by date split\"] = model\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "inp = L.Input(shape = (len(features),))\n",
    "#x = L.BatchNormalization()(inp)\n",
    "#x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(64)(inp)\n",
    "x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(32)(x)\n",
    "x = L.Dense(1)(x)\n",
    "out = L.Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "    metrics = tf.keras.metrics.AUC(name = \"AUC\")\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "    validation_data=(\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][\"action\"]\n",
    "    ),\n",
    "    epochs=1000, \n",
    "    batch_size=8*1024,\n",
    "    callbacks=[],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "tf_models[\"mlp\"] = model\n",
    "K.backend.clear_session()\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [],
   "source": [
    "extended_features = features[:]\n",
    "counter = 1\n",
    "for name, model in catboost_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = model.predict(df[features])\n",
    "    model.save_model(output_data_path + \"catboost_model_\" + str(counter) + \".cbm\")\n",
    "    counter += 1\n",
    "for name, model in tf_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = apply_tf_model(df[features], model)\n",
    "    model.save(output_data_path + \"tf_model_\" + str(counter) + \".h5\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8pqwhx445u8tpd2230as8"
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=2000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=False,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_2\"]][extended_features],\n",
    "        label=df[df[\"is_train_2\"]][\"action\"],\n",
    "        weight=df[df[\"is_train_2\"]][\"weight\"],\n",
    "        group_id=df[df[\"is_train_2\"]][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_2\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_train_1\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_val\"]], model, extended_features)\n",
    "estimate_model(df, model, extended_features)\n",
    "feature_importances(model, 5)\n",
    "model.save_model(output_data_path + \"model.cbm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notebookId": "8766bdf2-c6f6-4695-98e2-3af663e5fd96",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
