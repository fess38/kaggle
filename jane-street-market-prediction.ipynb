{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "kviks2l4hc3eyt87l6nfb"
   },
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "okba3s2j2ka81pejlmxh1o"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../input\n",
    "mkdir -p ../output\n",
    "cd ../input\n",
    "\n",
    "export KAGGLE_USERNAME=\"fess38\"\n",
    "export KAGGLE_KEY=\"071966146ec1ebef62023a5efa0574b1\"\n",
    "kaggle competitions download -c jane-street-market-prediction\n",
    "\n",
    "unzip jane-street-market-prediction.zip\n",
    "rm jane-street-market-prediction.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xm2d1ukqreky9p0d2g04fp"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "gyc0u5edjuizta8vsl8hvp",
    "execution_id": "9e442bba-f145-4cea-afdd-a772355bb1b4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "o37qnzj77wrdfxqvc6e1jv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import sum_models, CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from catboost.utils import get_gpu_device_count\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "z4g07mdqgheyzvtpn92a5g"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "mpl.rcParams[\"figure.figsize\"] = (11, 5)\n",
    "mpl.rcParams[\"figure.dpi\"]= 100\n",
    "mpl.rcParams[\"lines.linewidth\"] = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2l168l4ogre8w2j8m5wc49"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "tt1yhlytdrwu42ypeukol"
   },
   "outputs": [],
   "source": [
    "input_data_path = \"../input/\"\n",
    "output_data_path = \"../output/\"\n",
    "features = [\"feature_\" + str(i) for i in range(130)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qywtctpvhzkeomdx3why4o"
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "52jqd6zjwdp25uc2uakva2"
   },
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1skv8wzlynspw13r9lrpn"
   },
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def utility_score(date, weight, resp, action):\n",
    "    pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(pi) / np.sqrt(np.sum(pi**2)) * np.sqrt(250 / len(pi))\n",
    "    return int(min(max(t, 0), 6) * np.sum(pi))\n",
    "\n",
    "def split_df(df, date_splits):\n",
    "    for name, interval in date_splits.items():\n",
    "        df[\"is_\" + name] = df[\"date\"].apply(lambda x: x >= interval[0] and x <= interval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importances(model, top_n=20):\n",
    "    values = sorted(list(zip(model.feature_names_, model.feature_importances_)), key=lambda x: -x[1])\n",
    "    for value in values[:top_n]:\n",
    "        print(value[0], \": \", str(round(value[1], 2)))\n",
    "\n",
    "def estimate_model(df, model, features=features, threshold=0, print_result=True):\n",
    "    expected_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        df[\"action\"].values\n",
    "    )\n",
    "    actual_score = utility_score(\n",
    "        df[\"date\"].values,\n",
    "        df[\"weight\"].values,\n",
    "        df[\"resp\"].values,\n",
    "        (model.predict(df[features], prediction_type=\"RawFormulaVal\") > threshold).astype(int)\n",
    "    )\n",
    "    share = round(actual_score / expected_score, 2)\n",
    "    if print_result:\n",
    "        print(expected_score, actual_score, share)\n",
    "    return actual_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tf_model(df, model):\n",
    "    return model(df.values, training=False).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "23qjvnnz2p1sslb8os8379"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mfficxrfsda57fsipqij7n"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_data_path + \"train.csv\")\n",
    "df = df.astype({c: np.float32 for c in df.select_dtypes(include=\"float64\").columns})\n",
    "\n",
    "features_info = pd.read_csv(input_data_path + \"features.csv\")\n",
    "features_info.set_index(keys=[\"feature\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action\"] = (df[\"resp\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "df[\"action\"] = (\n",
    "    (df[\"resp\"] > threshold)\n",
    "    & (df[\"resp_1\"] > threshold)\n",
    "    & (df[\"resp_2\"] > threshold)\n",
    "    & (df[\"resp_3\"] > threshold)\n",
    "    & (df[\"resp_4\"] > threshold)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "422axp6sncgt4g88b7x6w"
   },
   "source": [
    "#### Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qgfdlsoo9h7w61paq1fvn"
   },
   "outputs": [],
   "source": [
    "def fillna_mean(df):\n",
    "    features_mean = df[features].mean()\n",
    "    df[features] = df[features].fillna(features_mean)\n",
    "    with open(output_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jnjpp98ivlflw88fk4qsj"
   },
   "outputs": [],
   "source": [
    "def fillna_ffill(df):\n",
    "    df[features] = df[features].fillna(method = \"ffill\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "90z64q083eagln57rb5su"
   },
   "outputs": [],
   "source": [
    "def fillna_mean_by_feature_0(df):\n",
    "    features_mean = df[features].groupby(\"feature_0\").mean()\n",
    "    features_mean[\"feature_0\"] = features_mean.index\n",
    "    df.sort_values(by=\"feature_0\", inplace=True)\n",
    "    df[features] = pd.concat([\n",
    "        df[df[\"feature_0\"] == -1][features].fillna(features_mean.loc[-1]),\n",
    "        df[df[\"feature_0\"] == 1][features].fillna(features_mean.loc[1])\n",
    "    ])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    with open(output_data_path + \"features_mean.pkl\", \"wb\") as f:\n",
    "        pickle.dump(features_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "6bmwkg795e93xd162opb6x"
   },
   "outputs": [],
   "source": [
    "fillna_mean_by_feature_0(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "sipnehvddlc5oea5ywtrvs"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_models = {}\n",
    "tf_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rnd\"] = np.random.rand(len(df))\n",
    "date_to_index = {}\n",
    "dates = list(set(df[\"date\"].values))\n",
    "np.random.shuffle(dates)\n",
    "for i, date in enumerate(dates):\n",
    "    date_to_index[date] = i\n",
    "df[\"order_id\"] = df[\"date\"].apply(lambda x: date_to_index[x])\n",
    "df.sort_values(by=[\"order_id\", \"rnd\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End2End Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = {\n",
    "    \"train\": [0, 449],\n",
    "    \"val\": [450, 499]\n",
    "}\n",
    "split_df(df, date_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = ParameterGrid({\n",
    "    \"iterations\": [2000],\n",
    "    \"learning_rate\": [0.005, 0.01, 0.03, 0.05],\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 10],\n",
    "    \"depth\": [8, 10, 12, 14, 16],\n",
    "    \"random_strength\": [1],\n",
    "    \"bagging_temperature\": [1],\n",
    "    \"border_count\": [128],\n",
    "    \"grow_policy\": [\"SymmetricTree\"],\n",
    "    \"use_weight\": [1, 0],\n",
    "    \"use_group_id\": [1, 0]\n",
    "})\n",
    "params_grid = sorted(list(params_grid), key=lambda x: x[\"use_group_id\"])\n",
    "\n",
    "grid_search_result = []\n",
    "dates = list(set(df[df[\"is_train\"]][\"date\"].values))\n",
    "sorted_by_dates, sorted_randomly = False, False\n",
    "\n",
    "for params in tqdm(params_grid, desc=\"Params Tuning\"):\n",
    "    scores = []\n",
    "    if params[\"use_group_id\"] and not sorted_by_dates:\n",
    "        df.sort_values(by=[\"order_id\", \"rnd\"], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        sorted_by_dates = True\n",
    "        sorted_randomly = False\n",
    "    if not params[\"use_group_id\"] and not sorted_randomly:\n",
    "        df.sort_values(by=[\"rnd\"], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        sorted_by_dates = False\n",
    "        sorted_randomly = True\n",
    "\n",
    "    for i in range(3):\n",
    "        train_dates, test_dates = train_test_split(dates, test_size=0.2, random_state=random_state+i)    \n",
    "        model = CatBoostClassifier(\n",
    "            loss_function=\"Logloss\",\n",
    "            iterations=params[\"iterations\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            random_seed=random_state,\n",
    "            l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "            use_best_model=True,\n",
    "            depth=params[\"depth\"],\n",
    "            random_strength=params[\"random_strength\"],\n",
    "            bagging_temperature=params[\"bagging_temperature\"],\n",
    "            border_count=params[\"border_count\"],\n",
    "            grow_policy=params[\"grow_policy\"],\n",
    "            auto_class_weights=\"Balanced\",\n",
    "            early_stopping_rounds=100,\n",
    "            task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X=Pool(\n",
    "                data=df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][features],\n",
    "                label=df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"action\"],\n",
    "                weight=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"weight\"]\n",
    "                    if params[\"use_weight\"] else None,\n",
    "                group_id=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(train_dates))][\"date\"]\n",
    "                    if params[\"use_group_id\"] else None\n",
    "            ),\n",
    "            eval_set=Pool(\n",
    "                data=df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][features],\n",
    "                label=df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"action\"],\n",
    "                weight=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"weight\"]\n",
    "                    if params[\"use_weight\"] else None,\n",
    "                group_id=\n",
    "                    df[(df[\"is_train\"]) & (df[\"date\"].isin(test_dates))][\"date\"]\n",
    "                    if params[\"use_group_id\"] else None\n",
    "            )\n",
    "        )\n",
    "        scores.append(estimate_model(df[df[\"is_val\"]], model, print_result=False))\n",
    "        pass\n",
    "    grid_search_result.append({\n",
    "        \"params\": params,\n",
    "        \"score\": sum(scores) / len(scores),\n",
    "        \"best_iteration\": model.best_iteration_,\n",
    "        \"best_score\": model.best_score_\n",
    "    })\n",
    "    grid_search_result = sorted(grid_search_result, key=lambda x: -x[\"score\"])\n",
    "    with open(output_data_path + \"grid_search_result.json\", \"w\") as f:\n",
    "        f.write(json.dumps(grid_search_result, indent=2))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": 60,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"depth\": 12,\n",
    "    \"random_strength\": 1,\n",
    "    \"bagging_temperature\": 1,\n",
    "    \"border_count\": 128,\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "    \"use_weight\": 0,\n",
    "    \"use_group_id\": 1\n",
    "}\n",
    "\n",
    "df.sort_values(\n",
    "    by=[\"order_id\", \"rnd\"] if params[\"use_group_id\"] else [\"rnd\"],\n",
    "    inplace=True\n",
    ")\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    iterations=params[\"iterations\"],\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "    depth=params[\"depth\"],\n",
    "    random_strength=params[\"random_strength\"],\n",
    "    bagging_temperature=params[\"bagging_temperature\"],\n",
    "    border_count=params[\"border_count\"],\n",
    "    grow_policy=params[\"grow_policy\"],\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[features],\n",
    "        label=df[\"action\"],\n",
    "        weight=df[\"weight\"] if params[\"use_weight\"] else None,\n",
    "        group_id=df[\"date\"] if params[\"use_group_id\"] else None\n",
    "    )\n",
    ")\n",
    "\n",
    "estimate_model(df[df[\"is_train\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "model.save_model(output_data_path + \"model.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[df[\"is_train\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=pd.concat([train[features].reset_index(drop=True), pd.DataFrame(encoder(train[features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        label=train[\"action\"].values,\n",
    "        weight=train[\"weight\"].values\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=pd.concat([test[features].reset_index(drop=True), pd.DataFrame(encoder(test[features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        label=test[\"action\"].values,\n",
    "        weight=test[\"weight\"].values\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(\n",
    "    df[df[\"is_val\"]][\"date\"].values,\n",
    "    df[df[\"is_val\"]][\"weight\"].values,\n",
    "    df[df[\"is_val\"]][\"resp\"].values,\n",
    "    (model.predict(\n",
    "        pd.concat([df[df[\"is_val\"]][features].reset_index(drop=True), pd.DataFrame(encoder(df[df[\"is_val\"]][features].values, training=False).numpy()).reset_index(drop=True)], axis=1),\n",
    "        prediction_type=\"RawFormulaVal\") > 0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = L.Input(130)\n",
    "encoded = L.BatchNormalization()(i)\n",
    "encoded = L.GaussianNoise(0.1)(encoded)\n",
    "encoded = L.Dense(64,activation='relu')(encoded)\n",
    "decoded = L.Dropout(0.2)(encoded)\n",
    "decoded = L.Dense(130, name='decoded')(decoded)\n",
    "x = L.Dense(64,activation='relu')(decoded)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(64,activation='relu')(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dropout(0.2)(x)    \n",
    "x = L.Dense(1, activation='sigmoid', name='label_output')(x)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n",
    "autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n",
    "\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.0001),loss={'decoded':'mse', 'label_output':'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(\n",
    "    df[df[\"is_train\"]][features],\n",
    "    (df[df[\"is_train\"]][features], df[df[\"is_train\"]][\"action\"]),\n",
    "    epochs=25,\n",
    "    batch_size=4096, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping('val_loss', patience=10,restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.3).astype(int),\n",
    "    df[df[\"is_val\"]][\"action\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.3).astype(int),\n",
    "    df[df[\"is_val\"]][\"action\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score(\n",
    "    df[df[\"is_val\"]][\"date\"].values,\n",
    "    df[df[\"is_val\"]][\"weight\"].values,\n",
    "    df[df[\"is_val\"]][\"resp\"].values,\n",
    "    #df[df[\"is_val\"]][\"action\"].values\n",
    "    (model.predict(df[df[\"is_val\"]][features], prediction_type=\"RawFormulaVal\") > -0.0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"is_val\"]].query(\"weight > 3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-stage model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to 2-stage train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = {\n",
    "    \"train_1\": [0, 224],\n",
    "    \"train_2\": [225, 449],\n",
    "    \"val\": [450, 499]\n",
    "}\n",
    "split_df(df, date_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df[(df[\"is_train_1\"])|(df[\"is_train_2\"])][features])\n",
    "df[features] = scaler.transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_data_path + \"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "p5e4iuqg0kgfaghj5jown8"
   },
   "source": [
    "#### Catboost with random train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[df[\"is_train_1\"]], test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=train[features],\n",
    "        label=train[\"action\"],\n",
    "        weight=train[\"weight\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=test[features],\n",
    "        label=test[\"action\"],\n",
    "        weight=test[\"weight\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"random split\"] = model\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost with date train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=1000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=True,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[train_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[train_idx][\"date\"]\n",
    "    ),\n",
    "    eval_set=Pool(\n",
    "        data=df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        label=df[df[\"is_train_1\"]].iloc[test_idx][\"action\"],\n",
    "        weight=df[df[\"is_train_1\"]].iloc[test_idx][\"weight\"],\n",
    "        group_id=df[df[\"is_train_1\"]].iloc[test_idx][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_1\"]], model)\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "estimate_model(df, model)\n",
    "feature_importances(model, 5)\n",
    "catboost_models[\"group by date split\"] = model\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_idx, test_idx in gss.split(X=df[df[\"is_train_1\"]].values, groups=df[df[\"is_train_1\"]][\"order_id\"].values):\n",
    "    pass\n",
    "\n",
    "inp = L.Input(shape = (len(features),))\n",
    "#x = L.BatchNormalization()(inp)\n",
    "#x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(64)(inp)\n",
    "x = L.Dropout(0.2)(x)\n",
    "x = L.Dense(32)(x)\n",
    "x = L.Dense(1)(x)\n",
    "out = L.Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "    metrics = tf.keras.metrics.AUC(name = \"AUC\")\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][features],\n",
    "    df[df[\"is_train_1\"]].iloc[train_idx][\"action\"],\n",
    "    validation_data=(\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][features],\n",
    "        df[df[\"is_train_1\"]].iloc[test_idx][\"action\"]\n",
    "    ),\n",
    "    epochs=1000, \n",
    "    batch_size=8*1024,\n",
    "    callbacks=[],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "estimate_model(df[df[\"is_val\"]], model)\n",
    "tf_models[\"mlp\"] = model\n",
    "K.backend.clear_session()\n",
    "del train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n7ym56b4zed6obnjnb5yk3"
   },
   "outputs": [],
   "source": [
    "extended_features = features[:]\n",
    "counter = 1\n",
    "for name, model in catboost_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = model.predict(df[features])\n",
    "    model.save_model(output_data_path + \"catboost_model_\" + str(counter) + \".cbm\")\n",
    "    counter += 1\n",
    "for name, model in tf_models.items():\n",
    "    extended_features.append(name)\n",
    "    df[name] = apply_tf_model(df[features], model)\n",
    "    model.save(output_data_path + \"tf_model_\" + str(counter) + \".h5\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8pqwhx445u8tpd2230as8"
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    custom_metric=[\"Precision\", \"Recall\", \"F1\"],\n",
    "    iterations=2000,\n",
    "    learning_rate=None,\n",
    "    random_seed=random_state,\n",
    "    l2_leaf_reg=3,\n",
    "    use_best_model=False,\n",
    "    depth=8,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    task_type=\"GPU\" if get_gpu_device_count() else \"CPU\",\n",
    "    metric_period=250,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=Pool(\n",
    "        data=df[df[\"is_train_2\"]][extended_features],\n",
    "        label=df[df[\"is_train_2\"]][\"action\"],\n",
    "        weight=df[df[\"is_train_2\"]][\"weight\"],\n",
    "        group_id=df[df[\"is_train_2\"]][\"date\"]\n",
    "    )\n",
    ")\n",
    "estimate_model(df[df[\"is_train_2\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_train_1\"]], model, extended_features)\n",
    "estimate_model(df[df[\"is_val\"]], model, extended_features)\n",
    "estimate_model(df, model, extended_features)\n",
    "feature_importances(model, 5)\n",
    "model.save_model(output_data_path + \"model.cbm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notebookId": "8766bdf2-c6f6-4695-98e2-3af663e5fd96",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
